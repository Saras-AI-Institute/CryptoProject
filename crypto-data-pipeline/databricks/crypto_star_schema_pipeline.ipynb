{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c72370",
   "metadata": {},
   "source": [
    "# Crypto Delta Lake Star Schema Pipeline\n",
    "\n",
    "End-to-end Databricks workflow that promotes normalized crypto pricing data into a star schema ready for BI consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118b37d",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "- Bronze: land raw CoinGecko JSON (one row per asset).\n",
    "- Silver: normalize into ingestion batches, cryptocurrencies, price snapshots, market metrics.\n",
    "- Gold: build star schema dimensions and fact table with Delta Lake merges.\n",
    "- Quality checks: confirm deduplication, row counts, and snapshot recency before publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d845ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "dbutils.widgets.text('raw_input_path', 'dbfs:/mnt/crypto/landing_zone/crypto_prices_sample.json', 'Raw JSON Path')\n",
    "dbutils.widgets.text('bronze_table', 'crypto_raw.prices_bronze', 'Bronze Table')\n",
    "dbutils.widgets.text('silver_db', 'crypto_silver', 'Silver Database')\n",
    "dbutils.widgets.text('gold_db', 'crypto_gold', 'Gold Database')\n",
    "dbutils.widgets.text('snapshot_interval_minutes', '5', 'Snapshot Interval (min)')\n",
    "\n",
    "landing_path = dbutils.widgets.get('raw_input_path')\n",
    "bronze_table = dbutils.widgets.get('bronze_table')\n",
    "silver_db = dbutils.widgets.get('silver_db')\n",
    "gold_db = dbutils.widgets.get('gold_db')\n",
    "snapshot_interval = int(dbutils.widgets.get('snapshot_interval_minutes'))\n",
    "snapshot_seconds = snapshot_interval * 60\n",
    "\n",
    "spark.sql('CREATE DATABASE IF NOT EXISTS crypto_raw')\n",
    "spark.sql(f'CREATE DATABASE IF NOT EXISTS {silver_db}')\n",
    "spark.sql(f'CREATE DATABASE IF NOT EXISTS {gold_db}')\n",
    "\n",
    "spark.conf.set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "spark.conf.set('spark.databricks.delta.schema.autoMerge.enabled', 'true')\n",
    "\n",
    "def upsert_to_delta(df, table_name, key_cols):\n",
    "    assignments = {col: F.col(f'source.{col}') for col in df.columns}\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        target = DeltaTable.forName(spark, table_name)\n",
    "        condition = ' AND '.join([f'target.{col} = source.{col}' for col in key_cols])\n",
    "        (target.alias('target').merge(df.alias('source'), condition)\n",
    "               .whenMatchedUpdate(set=assignments)\n",
    "               .whenNotMatchedInsert(values=assignments)\n",
    "               .execute())\n",
    "    else:\n",
    "        df.write.format('delta').mode('overwrite').saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "raw_df = (\n",
    "    spark.read.option('multiLine', True).json(landing_path)\n",
    "    .withColumn('ingested_at_ts', F.to_timestamp('ingested_at'))\n",
    "    .withColumn('record_count', F.size('records'))\n",
    ")\n",
    "\n",
    "bronze_rows = (\n",
    "    raw_df.select(\n",
    "        'ingested_at_ts',\n",
    "        'source',\n",
    "        'record_count',\n",
    "        F.posexplode_outer('records').alias('record_index', 'record')\n",
    "    )\n",
    "    .withColumn('landing_ts', F.current_timestamp())\n",
    "    .withColumn(\n",
    "        'record_hash',\n",
    "        F.sha2(\n",
    "            F.concat_ws('|', F.col('source'), F.col('ingested_at_ts').cast('string'), F.col('record.id')),\n",
    "            256\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "bronze_rows.write.format('delta').mode('append').saveAsTable(bronze_table)\n",
    "print(f'Bronze upsert complete: {bronze_rows.count()} raw rows written.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ba0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "bronze_df = spark.table(bronze_table)\n",
    "\n",
    "silver_enriched = (\n",
    "    bronze_df\n",
    "    .filter(F.col('record').isNotNull())\n",
    "    .withColumn('batch_id', F.sha2(F.concat_ws('|', F.col('source'), F.col('ingested_at_ts').cast('string')), 256))\n",
    "    .withColumn('last_updated_ts', F.to_timestamp(F.col('record.last_updated')))\n",
    "    .withColumn('ath_date_ts', F.to_timestamp(F.col('record.ath_date')))\n",
    "    .withColumn('atl_date_ts', F.to_timestamp(F.col('record.atl_date')))\n",
    "    .withColumn(\n",
    "        'snapshot_time',\n",
    "        F.when(\n",
    "            F.col('last_updated_ts').isNotNull(),\n",
    "            F.from_unixtime(\n",
    "                F.round(F.unix_timestamp(F.col('last_updated_ts')) / snapshot_seconds) * snapshot_seconds\n",
    "            ).cast('timestamp')\n",
    "        ).otherwise(F.col('ingested_at_ts'))\n",
    "    )\n",
    "    .withColumn(\n",
    "        'snapshot_key',\n",
    "        F.sha2(\n",
    "            F.concat_ws(\n",
    "                '|',\n",
    "                F.col('record.id'),\n",
    "                F.date_format('snapshot_time', 'yyyy-MM-dd HH:mm:ss'),\n",
    "                F.col('batch_id')\n",
    "            ),\n",
    "            256\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "silver_flat = silver_enriched.select(\n",
    "    'batch_id',\n",
    "    'source',\n",
    "    'ingested_at_ts',\n",
    "    'landing_ts',\n",
    "    'snapshot_time',\n",
    "    'snapshot_key',\n",
    "    'last_updated_ts',\n",
    "    'ath_date_ts',\n",
    "    'atl_date_ts',\n",
    "    F.col('record.id').alias('crypto_id'),\n",
    "    F.col('record.symbol').alias('symbol'),\n",
    "    F.col('record.name').alias('name'),\n",
    "    F.col('record.image').alias('image_url'),\n",
    "    F.col('record.current_price').alias('current_price'),\n",
    "    F.col('record.high_24h').alias('high_24h'),\n",
    "    F.col('record.low_24h').alias('low_24h'),\n",
    "    F.col('record.price_change_24h').alias('price_change_24h'),\n",
    "    F.col('record.price_change_percentage_24h').alias('price_change_pct_24h'),\n",
    "    F.col('record.ath').alias('ath'),\n",
    "    F.col('record.ath_change_percentage').alias('ath_change_pct'),\n",
    "    F.col('record.atl').alias('atl'),\n",
    "    F.col('record.atl_change_percentage').alias('atl_change_pct'),\n",
    "    F.col('record.market_cap').alias('market_cap'),\n",
    "    F.col('record.market_cap_rank').alias('market_cap_rank'),\n",
    "    F.col('record.fully_diluted_valuation').alias('fully_diluted_valuation'),\n",
    "    F.col('record.total_volume').alias('total_volume'),\n",
    "    F.col('record.market_cap_change_24h').alias('market_cap_change_24h'),\n",
    "    F.col('record.market_cap_change_percentage_24h').alias('market_cap_change_pct_24h'),\n",
    "    F.col('record.circulating_supply').alias('circulating_supply'),\n",
    "    F.col('record.total_supply').alias('total_supply'),\n",
    "    F.col('record.max_supply').alias('max_supply'),\n",
    "    F.col('record.roi.times').alias('roi_times'),\n",
    "    F.col('record.roi.currency').alias('roi_currency'),\n",
    "    F.col('record.roi.percentage').alias('roi_percentage')\n",
    ").filter(F.col('snapshot_key').isNotNull())\n",
    "\n",
    "ingestion_df = (\n",
    "    silver_flat.groupBy('batch_id', 'source', 'ingested_at_ts')\n",
    "    .agg(\n",
    "        F.count('*').alias('record_count'),\n",
    "        F.min('landing_ts').alias('landed_at')\n",
    "    )\n",
    "    .withColumnRenamed('ingested_at_ts', 'ingested_at')\n",
    ")\n",
    "\n",
    "cryptocurrencies_df = (\n",
    "    silver_flat.groupBy('crypto_id', 'symbol', 'name', 'image_url')\n",
    "    .agg(\n",
    "        F.min('snapshot_time').alias('first_snapshot_at'),\n",
    "        F.max('last_updated_ts').alias('last_source_update_at'),\n",
    "        F.max('landing_ts').alias('last_landed_at')\n",
    "    )\n",
    "    .withColumn('last_source_update_at', F.coalesce(F.col('last_source_update_at'), F.col('last_landed_at')))\n",
    "    .drop('last_landed_at')\n",
    ")\n",
    "\n",
    "price_snapshots_df = (\n",
    "    silver_flat.select(\n",
    "        'snapshot_key',\n",
    "        'crypto_id',\n",
    "        'batch_id',\n",
    "        'snapshot_time',\n",
    "        F.col('last_updated_ts').alias('last_updated_at'),\n",
    "        'current_price',\n",
    "        'high_24h',\n",
    "        'low_24h',\n",
    "        'price_change_24h',\n",
    "        'price_change_pct_24h',\n",
    "        'ath',\n",
    "        'ath_change_pct',\n",
    "        F.col('ath_date_ts').alias('ath_date'),\n",
    "        'atl',\n",
    "        'atl_change_pct',\n",
    "        F.col('atl_date_ts').alias('atl_date')\n",
    "    ).dropDuplicates(['snapshot_key'])\n",
    ")\n",
    "\n",
    "market_metrics_df = (\n",
    "    silver_flat.select(\n",
    "        'snapshot_key',\n",
    "        'market_cap',\n",
    "        'market_cap_rank',\n",
    "        'fully_diluted_valuation',\n",
    "        'total_volume',\n",
    "        'market_cap_change_24h',\n",
    "        'market_cap_change_pct_24h',\n",
    "        'circulating_supply',\n",
    "        'total_supply',\n",
    "        'max_supply',\n",
    "        'roi_times',\n",
    "        'roi_currency',\n",
    "        'roi_percentage'\n",
    "    ).dropDuplicates(['snapshot_key'])\n",
    ")\n",
    "\n",
    "upsert_to_delta(\n",
    "    ingestion_df.select('batch_id', 'source', 'ingested_at', 'record_count', 'landed_at'),\n",
    "    f'{silver_db}.ingestion_batches',\n",
    "    ['batch_id']\n",
    ")\n",
    "\n",
    "upsert_to_delta(\n",
    "    cryptocurrencies_df.select('crypto_id', 'symbol', 'name', 'image_url', 'first_snapshot_at', 'last_source_update_at'),\n",
    "    f'{silver_db}.cryptocurrencies',\n",
    "    ['crypto_id']\n",
    ")\n",
    "\n",
    "upsert_to_delta(\n",
    "    price_snapshots_df,\n",
    "    f'{silver_db}.price_snapshots',\n",
    "    ['snapshot_key']\n",
    ")\n",
    "\n",
    "upsert_to_delta(\n",
    "    market_metrics_df,\n",
    "    f'{silver_db}.market_metrics',\n",
    "    ['snapshot_key']\n",
    ")\n",
    "\n",
    "print('Silver layer refreshed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "spark.sql(f'''\n",
    "CREATE TABLE IF NOT EXISTS {gold_db}.dim_cryptocurrency (\n",
    "    crypto_key BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "    crypto_id STRING NOT NULL,\n",
    "    symbol STRING NOT NULL,\n",
    "    name STRING NOT NULL,\n",
    "    image_url STRING,\n",
    "    first_snapshot_at TIMESTAMP,\n",
    "    last_source_update_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP\n",
    ") USING DELTA\n",
    "TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')\n",
    "''')\n",
    "\n",
    "spark.sql(f'''\n",
    "CREATE TABLE IF NOT EXISTS {gold_db}.dim_source (\n",
    "    source_key BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "    source_name STRING NOT NULL,\n",
    "    first_ingested_at TIMESTAMP,\n",
    "    last_ingested_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP\n",
    ") USING DELTA\n",
    "TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')\n",
    "''')\n",
    "\n",
    "spark.sql(f'''\n",
    "CREATE TABLE IF NOT EXISTS {gold_db}.dim_date (\n",
    "    date_key INT,\n",
    "    full_date DATE,\n",
    "    day_of_month INT,\n",
    "    day_name STRING,\n",
    "    iso_week INT,\n",
    "    month_of_year INT,\n",
    "    month_name STRING,\n",
    "    quarter_of_year INT,\n",
    "    calendar_year INT,\n",
    "    is_weekend BOOLEAN\n",
    ") USING DELTA\n",
    "TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')\n",
    "''')\n",
    "\n",
    "spark.sql(f'''\n",
    "CREATE TABLE IF NOT EXISTS {gold_db}.dim_time (\n",
    "    time_key INT,\n",
    "    full_time STRING,\n",
    "    hour_24 INT,\n",
    "    minute_of_hour INT,\n",
    "    second_of_minute INT\n",
    ") USING DELTA\n",
    "TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')\n",
    "''')\n",
    "\n",
    "spark.sql(f'''\n",
    "CREATE TABLE IF NOT EXISTS {gold_db}.fact_crypto_price_metrics (\n",
    "    fact_id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "    business_key STRING NOT NULL,\n",
    "    crypto_key BIGINT NOT NULL,\n",
    "    date_key INT NOT NULL,\n",
    "    time_key INT NOT NULL,\n",
    "    source_key BIGINT NOT NULL,\n",
    "    batch_id STRING NOT NULL,\n",
    "    snapshot_timestamp TIMESTAMP NOT NULL,\n",
    "    last_updated_at TIMESTAMP NOT NULL,\n",
    "    current_price DECIMAL(20,8) NOT NULL,\n",
    "    high_24h DECIMAL(20,8),\n",
    "    low_24h DECIMAL(20,8),\n",
    "    price_change_24h DECIMAL(20,8),\n",
    "    price_change_pct_24h DECIMAL(10,5),\n",
    "    ath DECIMAL(20,8),\n",
    "    ath_change_pct DECIMAL(10,5),\n",
    "    atl DECIMAL(20,8),\n",
    "    atl_change_pct DECIMAL(10,5),\n",
    "    market_cap BIGINT,\n",
    "    market_cap_rank INT,\n",
    "    fully_diluted_valuation BIGINT,\n",
    "    total_volume BIGINT,\n",
    "    market_cap_change_24h BIGINT,\n",
    "    market_cap_change_pct_24h DECIMAL(10,5),\n",
    "    circulating_supply DECIMAL(30,8),\n",
    "    total_supply DECIMAL(30,8),\n",
    "    max_supply DECIMAL(30,8),\n",
    "    roi_times DECIMAL(20,8),\n",
    "    roi_percentage DECIMAL(20,8),\n",
    "    load_timestamp TIMESTAMP NOT NULL,\n",
    "    updated_at TIMESTAMP\n",
    ") USING DELTA\n",
    "TBLPROPERTIES ('delta.autoOptimize.autoCompact' = 'true')\n",
    "''')\n",
    "\n",
    "print('Gold schema ensured.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de1d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "silver_cryptos = spark.table(f'{silver_db}.cryptocurrencies')\n",
    "silver_batches = spark.table(f'{silver_db}.ingestion_batches')\n",
    "silver_prices = spark.table(f'{silver_db}.price_snapshots')\n",
    "silver_metrics = spark.table(f'{silver_db}.market_metrics')\n",
    "\n",
    "dim_crypto_df = silver_cryptos.select(\n",
    "    'crypto_id',\n",
    "    'symbol',\n",
    "    'name',\n",
    "    'image_url',\n",
    "    'first_snapshot_at',\n",
    "    'last_source_update_at',\n",
    "    F.current_timestamp().alias('updated_at')\n",
    ")\n",
    "\n",
    "dim_source_df = (\n",
    "    silver_batches.groupBy('source')\n",
    "    .agg(\n",
    "        F.min('ingested_at').alias('first_ingested_at'),\n",
    "        F.max('ingested_at').alias('last_ingested_at')\n",
    "    )\n",
    "    .withColumnRenamed('source', 'source_name')\n",
    "    .withColumn('updated_at', F.current_timestamp())\n",
    ")\n",
    "\n",
    "dim_date_df = (\n",
    "    silver_prices.select(F.to_date('snapshot_time').alias('full_date'))\n",
    "    .dropna()\n",
    "    .distinct()\n",
    "    .withColumn('date_key', F.year('full_date') * 10000 + F.month('full_date') * 100 + F.dayofmonth('full_date'))\n",
    "    .withColumn('day_of_month', F.dayofmonth('full_date'))\n",
    "    .withColumn('day_name', F.date_format('full_date', 'E'))\n",
    "    .withColumn('iso_week', F.weekofyear('full_date'))\n",
    "    .withColumn('month_of_year', F.month('full_date'))\n",
    "    .withColumn('month_name', F.date_format('full_date', 'MMM'))\n",
    "    .withColumn('quarter_of_year', F.quarter('full_date'))\n",
    "    .withColumn('calendar_year', F.year('full_date'))\n",
    "    .withColumn('is_weekend', F.dayofweek('full_date').isin(1, 7))\n",
    "    .select(\n",
    "        'date_key',\n",
    "        'full_date',\n",
    "        'day_of_month',\n",
    "        'day_name',\n",
    "        'iso_week',\n",
    "        'month_of_year',\n",
    "        'month_name',\n",
    "        'quarter_of_year',\n",
    "        'calendar_year',\n",
    "        F.col('is_weekend').cast('boolean').alias('is_weekend')\n",
    "    )\n",
    ")\n",
    "\n",
    "dim_time_df = (\n",
    "    silver_prices.select(F.date_trunc('minute', 'snapshot_time').alias('minute_ts'))\n",
    "    .dropna()\n",
    "    .distinct()\n",
    "    .withColumn('time_key', F.hour('minute_ts') * 100 + F.minute('minute_ts'))\n",
    "    .withColumn('full_time', F.date_format('minute_ts', 'HH:mm:ss'))\n",
    "    .withColumn('hour_24', F.hour('minute_ts'))\n",
    "    .withColumn('minute_of_hour', F.minute('minute_ts'))\n",
    "    .withColumn('second_of_minute', F.second('minute_ts'))\n",
    "    .select('time_key', 'full_time', 'hour_24', 'minute_of_hour', 'second_of_minute')\n",
    ")\n",
    "\n",
    "upsert_to_delta(dim_crypto_df, f'{gold_db}.dim_cryptocurrency', ['crypto_id'])\n",
    "upsert_to_delta(dim_source_df, f'{gold_db}.dim_source', ['source_name'])\n",
    "upsert_to_delta(dim_date_df, f'{gold_db}.dim_date', ['date_key'])\n",
    "upsert_to_delta(dim_time_df, f'{gold_db}.dim_time', ['time_key'])\n",
    "\n",
    "dim_crypto = spark.table(f'{gold_db}.dim_cryptocurrency').select('crypto_key', 'crypto_id')\n",
    "dim_source = spark.table(f'{gold_db}.dim_source').select('source_key', 'source_name')\n",
    "dim_date = spark.table(f'{gold_db}.dim_date').select('date_key', 'full_date')\n",
    "dim_time = spark.table(f'{gold_db}.dim_time').select('time_key', 'full_time')\n",
    "\n",
    "fact_df = (\n",
    "    silver_prices.alias('ps')\n",
    "    .join(silver_metrics.alias('mm'), 'snapshot_key', 'inner')\n",
    "    .join(silver_batches.alias('ib'), 'batch_id', 'inner')\n",
    "    .join(dim_crypto.alias('dc'), F.col('ps.crypto_id') == F.col('dc.crypto_id'), 'inner')\n",
    "    .join(dim_source.alias('ds'), F.col('ib.source') == F.col('ds.source_name'), 'inner')\n",
    "    .join(dim_date.alias('dd'), F.to_date('ps.snapshot_time') == F.col('dd.full_date'), 'inner')\n",
    "    .join(dim_time.alias('dt'), F.date_format('ps.snapshot_time', 'HH:mm:ss') == F.col('dt.full_time'), 'inner')\n",
    "    .select(\n",
    "        F.sha2(\n",
    "            F.concat_ws('|', F.col('dc.crypto_key'), F.date_format('ps.snapshot_time', 'yyyy-MM-dd HH:mm:ss')),\n",
    "            256\n",
    "        ).alias('business_key'),\n",
    "        F.col('dc.crypto_key'),\n",
    "        F.col('dd.date_key'),\n",
    "        F.col('dt.time_key'),\n",
    "        F.col('ds.source_key'),\n",
    "        F.col('ps.batch_id'),\n",
    "        F.col('ps.snapshot_time').alias('snapshot_timestamp'),\n",
    "        F.col('ps.last_updated_at'),\n",
    "        F.col('ps.current_price'),\n",
    "        F.col('ps.high_24h'),\n",
    "        F.col('ps.low_24h'),\n",
    "        F.col('ps.price_change_24h'),\n",
    "        F.col('ps.price_change_pct_24h'),\n",
    "        F.col('ps.ath'),\n",
    "        F.col('ps.ath_change_pct'),\n",
    "        F.col('ps.atl'),\n",
    "        F.col('ps.atl_change_pct'),\n",
    "        F.col('mm.market_cap'),\n",
    "        F.col('mm.market_cap_rank'),\n",
    "        F.col('mm.fully_diluted_valuation'),\n",
    "        F.col('mm.total_volume'),\n",
    "        F.col('mm.market_cap_change_24h'),\n",
    "        F.col('mm.market_cap_change_pct_24h'),\n",
    "        F.col('mm.circulating_supply'),\n",
    "        F.col('mm.total_supply'),\n",
    "        F.col('mm.max_supply'),\n",
    "        F.col('mm.roi_times'),\n",
    "        F.col('mm.roi_percentage'),\n",
    "        F.current_timestamp().alias('load_timestamp'),\n",
    "        F.current_timestamp().alias('updated_at')\n",
    "    )\n",
    ")\n",
    "\n",
    "upsert_to_delta(fact_df, f'{gold_db}.fact_crypto_price_metrics', ['business_key'])\n",
    "\n",
    "print('Gold layer refreshed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "fact_rows = spark.sql(f'SELECT COUNT(DISTINCT business_key) AS fact_rows FROM {gold_db}.fact_crypto_price_metrics')\n",
    "fact_rows.show()\n",
    "\n",
    "duplicates = spark.sql(f'''\n",
    "SELECT business_key, COUNT(*) AS cnt\n",
    "FROM {gold_db}.fact_crypto_price_metrics\n",
    "GROUP BY business_key\n",
    "HAVING COUNT(*) > 1\n",
    "''')\n",
    "duplicates.show()\n",
    "\n",
    "spark.sql(f'''\n",
    "SELECT d.symbol,\n",
    "       MAX(f.snapshot_timestamp) AS latest_snapshot,\n",
    "       COUNT(*) AS row_count\n",
    "FROM {gold_db}.fact_crypto_price_metrics f\n",
    "JOIN {gold_db}.dim_cryptocurrency d\n",
    "  ON f.crypto_key = d.crypto_key\n",
    "GROUP BY d.symbol\n",
    "ORDER BY latest_snapshot DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f46b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Data quality checks for negative magnitudes and missing critical attributes\n",
    "silver_prices = spark.table(f\"{silver_db}.price_snapshots\")\n",
    "silver_metrics = spark.table(f\"{silver_db}.market_metrics\")\n",
    "gold_fact = spark.table(f\"{gold_db}.fact_crypto_price_metrics\")\n",
    "\n",
    "dq_issues = []\n",
    "\n",
    "negative_price_count = silver_prices.filter(\"current_price <= 0 OR high_24h < 0 OR low_24h < 0\").count()\n",
    "if negative_price_count > 0:\n",
    "    dq_issues.append(f\"Found {negative_price_count} price records with invalid negative values.\")\n",
    "\n",
    "negative_metric_count = silver_metrics.filter(\"market_cap < 0 OR total_volume < 0 OR circulating_supply < 0\").count()\n",
    "if negative_metric_count > 0:\n",
    "    dq_issues.append(f\"Found {negative_metric_count} market metric records with negative magnitudes.\")\n",
    "\n",
    "null_fact_count = gold_fact.filter(\"snapshot_timestamp IS NULL OR current_price IS NULL\").count()\n",
    "if null_fact_count > 0:\n",
    "    dq_issues.append(f\"Found {null_fact_count} fact rows missing critical attributes.\")\n",
    "\n",
    "if dq_issues:\n",
    "    raise ValueError(\"; \".join(dq_issues))\n",
    "print(\"âœ… Data quality checks passed (no negative magnitudes, required fields present).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
